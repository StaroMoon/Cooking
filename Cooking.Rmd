---
title: "What's Cooking"
author: "RandomName"
date: "November 24, 2015"
output: ioslides_presentation
---
<style>
h1 {
  display: block;
    font-size: 4em;
    margin-top: 2em;
    margin-bottom: 2em;
    margin-left: 0;
    margin-right: 0;
    font-weight: bold;
    color: green
}
h3{
  display: block;
    font-size: 1.2em;
    margin-top: 0.67em;
    margin-bottom: 0.67em;
    margin-left: 0;
    margin-right: 0;
    font-weight: bold;
}
</style>
##Introduction
Objectives:
  
  - Classify the cuisine into several types
  
  - Compare different method to classify:
  
    - Decision tree
    
    - eXtreme Gradient boosting (Xgboost)
  
##Setup the Bench
Load all the libraries
```{r, message= FALSE}
library("dplyr")
library("tm")
library("caret")
library("jsonlite")
library("xgboost")
library("Matrix")
library("rpart")
library("ggvis")
```

##Load the files and save to variables
```{r}
train <- fromJSON("train.json", flatten = TRUE)
test <- fromJSON("test.json", flatten = TRUE)
```
![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/train.jpg)

##Plot graph from train
```{r, echo = F}
train %>%
  ggvis(~cuisine) %>%
  layer_bars(width = 1) %>%
  add_axis("x", title_offset = 50,
           properties = axis_props(
    labels = list(angle = 45, align = "left")
  )) %>%
  add_axis("y", title_offset = 50)
```

## Pre-processing data

```{r}
train$ingredients <- lapply(train$ingredients, tolower)
train$ingredients <- lapply(train$ingredients, 
                            function(x) gsub("[^a-z0-9 -]", "", x))

test$ingredients <- lapply(test$ingredients, tolower)
test$ingredients <- lapply(test$ingredients, 
                           function(x) gsub("[^a-z0-9 -]", "", x))
```

## Text-Mining

Create a corpus (collection of texts) of ingredient in train.json and test.json files
```{r}
corpus <- c(Corpus(VectorSource(train$ingredients)), 
            Corpus(VectorSource(test$ingredients)))
```

<center><h3>Create document term matrix</h3></center>

```{r}
dtm <- DocumentTermMatrix(corpus)
```
```{r,echo = F}
dtm
```

##Delete some sprase in matrix
```{r}
dtm <- removeSparseTerms(dtm, 1-3/nrow(dtm))
```
```{r,echo = F}
dtm
```

## Convert back
  convert back the document matrix to dataframe and add a new "cuisine" column
```{r, eval = F}
dtm <- as.data.frame(as.matrix(dtm))
dtm$cuisine <- as.factor(c(train$cuisine, rep("italian", nrow(test))))
```
![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/dtm_dataframe.jpg)

## Select data from dtm and store to new variables
```{r, eval = F}
n <- nrow(train)
train_test <- dtm[1:round(0.7 * n),]
test_test <- dtm[(round(0.7 * n) + 1):n, ]
dtm_train <- dtm[1:n,]
dtm_test <- dtm[-(1:n),]
```

##Create decision tree model
Build decision tree model and calculate accuracy of model
```{r, eval = F}
set.seed(1234)
tree <- rpart(cuisine ~ ., train_test, method = "class", 
              control = rpart.control(cp = 0.005))
pred_tree <- predict(tree, test_test, type = "class")
```
[Decision tree](https://raw.githubusercontent.com/StaroMoon/Cooking/master/tree.jpg)
[Confusion matrix](https://raw.githubusercontent.com/StaroMoon/Cooking/master/confusion_tree.png)

## Time to predict
Predict the data with the model
```{r, eval = F}
tree <- rpart(cuisine ~ ., dtm_train, method = "class", 
              control = rpart.control(cp = 0.005))
pred_tree <- predict(tree, dtm_test, type = "class")
```
![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/submit_t.png)

##Xgboost (eXtreme gradient boosting)
  * What is xgboost?
  * Why we choose this?
  
##What is xgboost?
  Xgboost is a R library. Gradient boosting is a machine learning technique , and what is gradient boosting?
  ![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/boosting_tree.jpg)
  
##Why xgboost?
  - Easy to use
  - Good result for most data sets
  - It's many tunable parameters
  - Multiple thread
  - Take low time to compute
  - Most of winner on kaggle use this library
  
[more info](https://www.youtube.com/watch?v=X47SGnTMZIU)
  
##Create Xgboost model
Build Xgboost model and calculate accuracy
```{r, eval = F}
dtrain <- xgb.DMatrix(Matrix(data.matrix(
                  train_test[,colnames(train_test) != "cuisine"])),
                  label = as.numeric(train_test$cuisine)-1)

xgb <- xgboost(dtrain, max.depth = 40, eta = 0.07, 
               nround = 1000, objective = "multi:softmax",
               num_class = 20)
```
```{r, eval = F}
pred_xgb <- predict(xgb, newdata = 
                      data.matrix(
                        test_test[,colnames(test_test) != "cuisine"]))
pred_string <- levels(train_test$cuisine)[pred_xgb+1]
```
[Confusion matrix](https://raw.githubusercontent.com/StaroMoon/Cooking/master/confusion_xgb.png)

## Time to predict
Predict the data with the model
```{r, eval = F}
dtrain <- xgb.DMatrix(Matrix(data.matrix(
                  dtm_train[,colnames(dtm_train) != "cuisine"])),
                  label = as.numeric(dtm_train$cuisine)-1)
xgb <- xgboost(dtrain, max.depth = 40, eta = 0.07, 
               nround = 1000, objective = "multi:softmax",
               num_class = 20)
```
```{r, eval = F}
pred_xgb <- predict(xgb, newdata = 
                      data.matrix(
                        dtm_test[,colnames(dtm_test) != "cuisine"]))
pred_string <- levels(dtm_train$cuisine)[pred_xgb+1]
```
![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/submit_xgb.png)

##Make submission file
```{r, eval = F}
sample_sub <- read.csv("sample_submission.csv")
submit_match <- cbind(as.data.frame(dtm_test$id), 
                      as.data.frame(pred_tree))
colnames(submit_match) <-c("id","cuisine")
write.csv(submit_match, file = "submit.csv", row.names=FALSE)
```

## Result

Decision tree -> 0.484 = 48.4 %
![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/submit_t.png)
Xgboost -> 0.797 = 79.7 % 
![](https://raw.githubusercontent.com/StaroMoon/Cooking/master/submit_xgb.png)

***
<h1><center> Thank you </center></h1>